{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "A8wHjpRbOfG8"
   },
   "source": [
    "**HOMEWORK**\n",
    "\n",
    "* **1bp** Use kernel/bias regularization to improve the behaviour of\n",
    "AlexNet on the CIFAR10 dataset, using the random initialization.\n",
    "Experiment with multiple\n",
    "[initializers](https://keras.io/api/layers/initializers/).\n",
    "Report your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, BatchNormalization, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "\n",
    "np.random.seed(1000)\n",
    "\n",
    "\n",
    "def create_alex_net(kernel_initializer, bias_initializer):\n",
    "    AlexNet = Sequential()\n",
    "    # block 1\n",
    "    AlexNet.add(Conv2D(filters=96, input_shape=(32,32,3), kernel_size=(11,11), strides=(4,4), padding='same'\n",
    "                       , kernel_initializer=kernel_initializer, bias_initializer=bias_initializer))\n",
    "    AlexNet.add(BatchNormalization())\n",
    "    AlexNet.add(Activation('relu'))\n",
    "    AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "    #block 2\n",
    "    AlexNet.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'\n",
    "                       , kernel_initializer=kernel_initializer, bias_initializer=bias_initializer))\n",
    "    AlexNet.add(BatchNormalization())\n",
    "    AlexNet.add(Activation('relu'))\n",
    "    AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "    #block 3\n",
    "    AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'\n",
    "                       , kernel_initializer=kernel_initializer, bias_initializer=bias_initializer))\n",
    "    AlexNet.add(BatchNormalization())\n",
    "    AlexNet.add(Activation('relu'))\n",
    "\n",
    "    #block 4\n",
    "    AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'\n",
    "                       , kernel_initializer=kernel_initializer, bias_initializer=bias_initializer))\n",
    "    AlexNet.add(BatchNormalization())\n",
    "    AlexNet.add(Activation('relu'))\n",
    "\n",
    "    #block 5\n",
    "    AlexNet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'\n",
    "                       , kernel_initializer=kernel_initializer, bias_initializer=bias_initializer))\n",
    "    AlexNet.add(BatchNormalization())\n",
    "    AlexNet.add(Activation('relu'))\n",
    "    AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "    AlexNet.add(Flatten())\n",
    "    AlexNet.add(Dense(4096, input_shape=(32,32,3,)\n",
    "                      , kernel_initializer=kernel_initializer, bias_initializer=bias_initializer))\n",
    "    AlexNet.add(BatchNormalization())\n",
    "    AlexNet.add(Activation('relu'))\n",
    "    AlexNet.add(Dropout(0.4))\n",
    "    AlexNet.add(Dense(4096\n",
    "                      , kernel_initializer=kernel_initializer, bias_initializer=bias_initializer))\n",
    "    AlexNet.add(BatchNormalization())\n",
    "    AlexNet.add(Activation('relu'))\n",
    "    AlexNet.add(Dropout(0.4))\n",
    "    AlexNet.add(Dense(1000\n",
    "                      , kernel_initializer=kernel_initializer, bias_initializer=bias_initializer))\n",
    "    AlexNet.add(BatchNormalization())\n",
    "    AlexNet.add(Activation('relu'))\n",
    "    AlexNet.add(Dropout(0.4))\n",
    "    AlexNet.add(Dense(10\n",
    "                      , kernel_initializer=kernel_initializer, bias_initializer=bias_initializer))\n",
    "    AlexNet.add(BatchNormalization())\n",
    "    AlexNet.add(Activation('softmax'))\n",
    "\n",
    "    AlexNet.compile(loss = keras.losses.categorical_crossentropy, optimizer= 'adam', metrics=['accuracy'])\n",
    "    return AlexNet\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "TDfSk9mfOfHB"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_20 (Conv2D)          (None, 8, 8, 96)          34944     \n",
      "                                                                 \n",
      " batch_normalization_34 (Bat  (None, 8, 8, 96)         384       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_34 (Activation)  (None, 8, 8, 96)          0         \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 4, 4, 96)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 4, 4, 256)         614656    \n",
      "                                                                 \n",
      " batch_normalization_35 (Bat  (None, 4, 4, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_35 (Activation)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPoolin  (None, 2, 2, 256)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 2, 2, 384)         885120    \n",
      "                                                                 \n",
      " batch_normalization_36 (Bat  (None, 2, 2, 384)        1536      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_36 (Activation)  (None, 2, 2, 384)         0         \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 2, 2, 384)         1327488   \n",
      "                                                                 \n",
      " batch_normalization_37 (Bat  (None, 2, 2, 384)        1536      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_37 (Activation)  (None, 2, 2, 384)         0         \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, 2, 2, 256)         884992    \n",
      "                                                                 \n",
      " batch_normalization_38 (Bat  (None, 2, 2, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_38 (Activation)  (None, 2, 2, 256)         0         \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  (None, 1, 1, 256)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 4096)              1052672   \n",
      "                                                                 \n",
      " batch_normalization_39 (Bat  (None, 4096)             16384     \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_39 (Activation)  (None, 4096)              0         \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 4096)              16781312  \n",
      "                                                                 \n",
      " batch_normalization_40 (Bat  (None, 4096)             16384     \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_40 (Activation)  (None, 4096)              0         \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 1000)              4097000   \n",
      "                                                                 \n",
      " batch_normalization_41 (Bat  (None, 1000)             4000      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_41 (Activation)  (None, 1000)              0         \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 10)                10010     \n",
      "                                                                 \n",
      " batch_normalization_42 (Bat  (None, 10)               40        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_42 (Activation)  (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,730,506\n",
      "Trainable params: 25,709,350\n",
      "Non-trainable params: 21,156\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "AlexNet = create_alex_net('zeros', 'zeros')\n",
    "AlexNet.summary()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "19gt-KtzOfHD",
    "outputId": "d355a430-9aa7-4ac1-a38f-181443dcd8e5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# load the CIFAR 10 data\n",
    "from keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def load_cifar10_data():\n",
    "  (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "  y_train = to_categorical(y_train)\n",
    "  y_test = to_categorical(y_test)\n",
    "  return x_train, x_test, y_train, y_test\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "a2RpZVC0OfHF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 1.5642 - accuracy: 0.4422\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.3173 - accuracy: 0.5382\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.1742 - accuracy: 0.5932\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.0542 - accuracy: 0.6376\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.9492 - accuracy: 0.6743\n",
      "random_normal 1.1380503177642822 0.6111999750137329\n",
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 1.5816 - accuracy: 0.4342\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.3341 - accuracy: 0.5307\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.1921 - accuracy: 0.5846\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.0709 - accuracy: 0.6293\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.9677 - accuracy: 0.6674\n",
      "random_uniform 1.3054465055465698 0.5561000108718872\n",
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 1.5673 - accuracy: 0.4424\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.3177 - accuracy: 0.5363\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.1752 - accuracy: 0.5919\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.0579 - accuracy: 0.6354\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.9520 - accuracy: 0.6760\n",
      "truncated_normal 1.3752315044403076 0.5264000296592712\n",
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 2.3028 - accuracy: 0.0992\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 21s 14ms/step - loss: 2.3028 - accuracy: 0.0995\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 21s 14ms/step - loss: 2.3028 - accuracy: 0.0990\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 21s 14ms/step - loss: 2.3028 - accuracy: 0.0986\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 21s 14ms/step - loss: 2.3028 - accuracy: 0.0979\n",
      "zeros 2.3109729290008545 0.10000000149011612\n",
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 2.2574 - accuracy: 0.1469\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 2.1306 - accuracy: 0.1885\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 2.0928 - accuracy: 0.1909\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 2.0724 - accuracy: 0.1922\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 2.0554 - accuracy: 0.1901\n",
      "ones 2.0341594219207764 0.19539999961853027\n",
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 1.6226 - accuracy: 0.4135\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.3672 - accuracy: 0.5154\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.2390 - accuracy: 0.5641\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.1290 - accuracy: 0.6065\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.0271 - accuracy: 0.6431\n",
      "glorot_normal 1.1748121976852417 0.5884000062942505\n",
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 1.6080 - accuracy: 0.4195\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.3509 - accuracy: 0.5238\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.2180 - accuracy: 0.5755\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.1089 - accuracy: 0.6128\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.0089 - accuracy: 0.6494\n",
      "glorot_uniform 1.6946511268615723 0.43959999084472656\n",
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 1.6310 - accuracy: 0.4089\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.3709 - accuracy: 0.5125\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.2357 - accuracy: 0.5639\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.1241 - accuracy: 0.6060\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.0253 - accuracy: 0.6423\n",
      "glorot_uniform 1.1936275959014893 0.5752999782562256\n",
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 1.5701 - accuracy: 0.4407\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.3168 - accuracy: 0.5367\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.1780 - accuracy: 0.5895\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.0642 - accuracy: 0.6306\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.9620 - accuracy: 0.6690\n",
      "he_uniform 1.1416836977005005 0.6007000207901001\n",
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 1.5676 - accuracy: 0.4401\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.3123 - accuracy: 0.5394\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.1756 - accuracy: 0.5928\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.0654 - accuracy: 0.6294\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.9626 - accuracy: 0.6708\n",
      "he_normal 1.2005150318145752 0.5823000073432922\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = load_cifar10_data()\n",
    "initializers = [\n",
    "    'random_normal',\n",
    "    'random_uniform',\n",
    "    'truncated_normal',\n",
    "    'zeros',\n",
    "    'ones',\n",
    "    'glorot_normal',\n",
    "    'glorot_uniform',\n",
    "    'glorot_uniform',\n",
    "    'he_uniform',\n",
    "    'he_normal',\n",
    "]\n",
    "for init in initializers:\n",
    "    AlexNet = create_alex_net(init, init)\n",
    "    AlexNet.fit(X_train, Y_train, epochs=5)\n",
    "    loss, acc = AlexNet.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(init, loss, acc)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rIaaE8-3OfHF",
    "outputId": "5a010643-5d8d-48d4-bd29-85d848b1dd17"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that random_normal and he_uniform gave the best results, but they were not significantly better.\n",
    "Using a zero  initializer would make the network not trainable, and for a network initialized with ones,\n",
    "it will need a long warm up period to be able to adjust the weights."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "IsyWGIGHjxKe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "t9kse4oJw5Oe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "* **1bp** Perform K-Fold CV on a sequence of models, aiming for polynomial regression over a dataset of synthetically generated data. Are the complexe models chosen over the simpler models? Why/ why not?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "iDtgJXAWOfHG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "cPC66glKOfHG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* **1bp** Fit ResNet50 on CIFAR10 using other types of optimizations techniques (RMSProp, AdaGrad). Can you explain the results by the specific of the algorithms?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "-6v1Q8tOOfHH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def resnet50(inputs):\n",
    "\n",
    "  resnet50 = tf.keras.applications.resnet.ResNet50(input_shape=(32, 32, 3),\n",
    "                                               include_top=False,\n",
    "                                               weights=None)(inputs)\n",
    "  return resnet50\n",
    "\n",
    "\n",
    "def classifier(inputs):\n",
    "    x = tf.keras.layers.Flatten()(inputs)\n",
    "    x = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"classification\")(x)\n",
    "    return x\n",
    "\n",
    "def final_model(inputs):\n",
    "    resnet = resnet50(inputs)\n",
    "    output = classifier(resnet)\n",
    "    return output\n",
    "\n",
    "def model(optimizer):\n",
    "    inputs = tf.keras.layers.Input(shape=(32,32,3))\n",
    "    output = final_model(inputs) \n",
    "    model = tf.keras.Model(inputs=inputs, outputs = output)\n",
    " \n",
    "    model.compile(optimizer=optimizer, \n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                metrics = ['accuracy'])\n",
    "  \n",
    "    return model\n"
   ],
   "metadata": {
    "id": "XWavmLxPWgxi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 89s 50ms/step - loss: 1.9024 - accuracy: 0.3133\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 77s 49ms/step - loss: 1.4898 - accuracy: 0.4590\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 76s 49ms/step - loss: 1.3076 - accuracy: 0.5336\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 77s 49ms/step - loss: 1.1677 - accuracy: 0.5877\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 77s 49ms/step - loss: 1.1747 - accuracy: 0.6051\n",
      "313/313 [==============================] - 5s 14ms/step - loss: 2.1364 - accuracy: 0.4145\n",
      "[2.1364195346832275, 0.41449999809265137]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, Y_train, Y_test = load_cifar10_data()\n",
    "\n",
    "resnet50 = model('rmsprop')\n",
    "resnet50.fit(X_train, Y_train, epochs=5, verbose=1)\n",
    "print(resnet50.evaluate(X_test, Y_test, verbose=1))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7UVn6izTOfHI",
    "outputId": "e1d54973-dc52-4008-bb25-e525b186b715"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 68s 40ms/step - loss: 2.2034 - accuracy: 0.4117\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 63s 40ms/step - loss: 2.0141 - accuracy: 0.5079\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 63s 40ms/step - loss: 1.8130 - accuracy: 0.5406\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 63s 40ms/step - loss: 1.6230 - accuracy: 0.5632\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 64s 41ms/step - loss: 1.4738 - accuracy: 0.5755\n",
      "313/313 [==============================] - 5s 14ms/step - loss: 1.4140 - accuracy: 0.5829\n",
      "[1.4140031337738037, 0.5828999876976013]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, Y_train, Y_test = load_cifar10_data()\n",
    "\n",
    "resnet50 = model('adagrad')\n",
    "resnet50.fit(X_train, Y_train, epochs=5, verbose=1)\n",
    "print(resnet50.evaluate(X_test, Y_test, verbose=1))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UN7_c0W9OfHI",
    "outputId": "ab12132a-7590-44dd-fdfa-4f8948ee4e14"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adagrad:\n",
    " * adapts depending on the size of the gradient\n",
    " * It is inverse dependant on the size of the gradient\n",
    " * based on recently past gradients\n",
    " * advantages features that appear rarely\n",
    " * However, it can become  blocked on plateaus\n",
    "\n",
    "RMSprop:\n",
    " * same as Adagrad, but instead of keeping track of past gradients, it uses an exponential running average\n",
    " * on each iteration it divides the learning rate by exponential running average gradient\n",
    "\n",
    "\n",
    "As we can see, adagrad performs better than rmsprop in the short term. This may be due to the fact that\n",
    "unlike RMSProp, which uses an exponential running average, the past gradients used\n",
    "by adagrad help adjust the learning rate better, trading memory for convergence.\n",
    "Also, it could be that Adagrad performed better in the first few epochs because\n",
    "it didn't yet encounter plateaus."
   ],
   "metadata": {
    "collapsed": false,
    "id": "IOoGZQuWjxKg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "9BzLGS7_w3aG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "* **1bp** Use the simple LeNet demo to do transfer learning, classifying on the STL 10 dataset. Compare the transfer learning model performance with the one randomly initialized.\n"
   ],
   "metadata": {
    "id": "OgGNp5yMfLDD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install extra-keras-datasets"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FovnW0rsepx9",
    "outputId": "20a22944-2144-47de-f921-95a376edaeff"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting extra-keras-datasets\n",
      "  Downloading extra_keras_datasets-1.2.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from extra-keras-datasets) (1.3.5)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from extra-keras-datasets) (1.21.6)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from extra-keras-datasets) (1.0.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from extra-keras-datasets) (1.4.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->extra-keras-datasets) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->extra-keras-datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->extra-keras-datasets) (1.15.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->extra-keras-datasets) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->extra-keras-datasets) (3.1.0)\n",
      "Installing collected packages: extra-keras-datasets\n",
      "Successfully installed extra-keras-datasets-1.2.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:root:Loading dataset = stl-10\n",
      "WARNING:root:Please cite the following paper when using or referencing this Extra Keras Dataset:\n",
      "WARNING:root:Coates, A., Ng, A., & Lee, H. (2011, June). An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics (pp. 215-223).Retrieved from http://cs.stanford.edu/~acoates/papers/coatesleeng_aistats_2011.pdf\n"
     ]
    }
   ],
   "source": [
    "from extra_keras_datasets import stl10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "(input_train, target_train), (input_test, target_test) = stl10.load_data()\n",
    "target_train = target_train - 1\n",
    "target_test = target_test  -1\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QtiJiaWcOfHJ",
    "outputId": "62f0c161-1c83-41e5-da3b-b4df69fc9444"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "target_train = to_categorical(target_train)\n",
    "target_test = to_categorical(target_test)"
   ],
   "metadata": {
    "id": "xK9zwOpmtRH6"
   },
   "execution_count": 58,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "input_train.shape, target_train.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_IDFFzsTlrf_",
    "outputId": "eee20045-98d6-4210-def1-2ec11e41ea32"
   },
   "execution_count": 59,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((5000, 96, 96, 3), (5000, 10))"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Conv2D, AveragePooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def get_mnist_data():\n",
    "  (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "  X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\n",
    "  X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))\n",
    "  Y_train = to_categorical(Y_train) # use one-hot encoding\n",
    "  Y_test = to_categorical(Y_test)\n",
    "  \n",
    "  return X_train, Y_train, X_test, Y_test\n",
    "\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ccALW7bBjxKh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from keras.layers import AveragePooling2D, InputLayer\n",
    "\n",
    "def create_lenet():\n",
    "    model = keras.Sequential()\n",
    "    model.add(InputLayer(input_shape=(28,28,1)))\n",
    "    model.add(Conv2D(filters=6, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(AveragePooling2D())\n",
    "    model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(AveragePooling2D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=120, activation='relu'))\n",
    "    model.add(Dense(units=84, activation='relu'))\n",
    "    model.add(Dense(units=10, activation = 'softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ],
   "metadata": {
    "id": "fmjl2f14e26M"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_on_mnist(model):\n",
    "    X_train, Y_train, _, _ = get_mnist_data()\n",
    "    model.fit(X_train, Y_train, epochs=10)\n",
    "    return model"
   ],
   "metadata": {
    "id": "9hHw6XvygBTU"
   },
   "execution_count": 70,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_for_stl_10(train):\n",
    "    model = create_lenet()\n",
    "    if train:\n",
    "        train_on_mnist(model)\n",
    "    new_model = keras.Sequential()\n",
    "    new_model.add(InputLayer(input_shape=(96,96,3)))\n",
    "    new_model.add(Conv2D(filters=6, kernel_size=(5, 5), activation='relu'))\n",
    "    new_model.add(AveragePooling2D(strides=(2, 2)))\n",
    "    new_model.add(Conv2D(filters=6, kernel_size=(5, 5), activation='relu'))\n",
    "    new_model.add(Conv2D(filters=6, kernel_size=(5, 5), activation='relu'))\n",
    "    new_model.add(Conv2D(filters=6, kernel_size=(5, 5), activation='relu'))\n",
    "    new_model.add(Conv2D(filters=6, kernel_size=(5, 5), activation='relu'))\n",
    "    new_model.add(Conv2D(filters=1, kernel_size=(3, 3), activation='relu'))\n",
    "    for layer in model.layers[:]:\n",
    "        new_model.add(layer)\n",
    "    new_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return new_model"
   ],
   "metadata": {
    "id": "1YOYLJPwpb7k"
   },
   "execution_count": 71,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = create_for_stl_10(True)\n",
    "model.fit(input_train, target_train, epochs=20)\n",
    "model.evaluate(input_test, target_test)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eD_EpbCHqW5-",
    "outputId": "4dd457a2-3b9c-4af6-fb75-da822c95491c"
   },
   "execution_count": 72,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2675 - accuracy: 0.9424\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0681 - accuracy: 0.9798\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0530 - accuracy: 0.9837\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0418 - accuracy: 0.9872\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0374 - accuracy: 0.9879\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0302 - accuracy: 0.9910\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0254 - accuracy: 0.9918\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0212 - accuracy: 0.9933\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0201 - accuracy: 0.9937\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0183 - accuracy: 0.9943\n",
      "Epoch 1/20\n",
      "157/157 [==============================] - 2s 8ms/step - loss: 2.2975 - accuracy: 0.1220\n",
      "Epoch 2/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 2.1760 - accuracy: 0.1908\n",
      "Epoch 3/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 1.9797 - accuracy: 0.2500\n",
      "Epoch 4/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 1.8618 - accuracy: 0.2890\n",
      "Epoch 5/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 1.7712 - accuracy: 0.3174\n",
      "Epoch 6/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 1.7003 - accuracy: 0.3544\n",
      "Epoch 7/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 1.6426 - accuracy: 0.3770\n",
      "Epoch 8/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 1.5633 - accuracy: 0.4082\n",
      "Epoch 9/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 1.4751 - accuracy: 0.4388\n",
      "Epoch 10/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 1.3973 - accuracy: 0.4696\n",
      "Epoch 11/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 1.2958 - accuracy: 0.5152\n",
      "Epoch 12/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 1.1941 - accuracy: 0.5586\n",
      "Epoch 13/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 1.1004 - accuracy: 0.5968\n",
      "Epoch 14/20\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 1.0188 - accuracy: 0.6220\n",
      "Epoch 15/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 0.8879 - accuracy: 0.6806\n",
      "Epoch 16/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 0.8116 - accuracy: 0.7082\n",
      "Epoch 17/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 0.6918 - accuracy: 0.7512\n",
      "Epoch 18/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 0.6006 - accuracy: 0.7794\n",
      "Epoch 19/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 0.5732 - accuracy: 0.8004\n",
      "Epoch 20/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 0.4919 - accuracy: 0.8222\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9704 - accuracy: 0.2924\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[3.970431089401245, 0.29237499833106995]"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model = create_for_stl_10(False)\n",
    "model.fit(input_train, target_train, epochs=20)\n",
    "model.evaluate(input_test, target_test)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zvz4FEN4uFB4",
    "outputId": "dca59ce4-c75b-460b-aa82-067f77457d59"
   },
   "execution_count": 73,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "157/157 [==============================] - 2s 8ms/step - loss: 2.3052 - accuracy: 0.0972\n",
      "Epoch 2/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 2.3029 - accuracy: 0.0966\n",
      "Epoch 3/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 2.3030 - accuracy: 0.0956\n",
      "Epoch 4/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 2.3031 - accuracy: 0.0948\n",
      "Epoch 5/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 2.3009 - accuracy: 0.1008\n",
      "Epoch 6/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 2.2974 - accuracy: 0.0988\n",
      "Epoch 7/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 2.2440 - accuracy: 0.1652\n",
      "Epoch 8/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 2.0055 - accuracy: 0.2404\n",
      "Epoch 9/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 1.8094 - accuracy: 0.2902\n",
      "Epoch 10/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 1.7281 - accuracy: 0.3208\n",
      "Epoch 11/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 1.6750 - accuracy: 0.3538\n",
      "Epoch 12/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 1.6328 - accuracy: 0.3712\n",
      "Epoch 13/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 1.5570 - accuracy: 0.3970\n",
      "Epoch 14/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 1.4929 - accuracy: 0.4344\n",
      "Epoch 15/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 1.4637 - accuracy: 0.4502\n",
      "Epoch 16/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 1.4005 - accuracy: 0.4716\n",
      "Epoch 17/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 1.3542 - accuracy: 0.4900\n",
      "Epoch 18/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 1.2780 - accuracy: 0.5240\n",
      "Epoch 19/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 1.2096 - accuracy: 0.5480\n",
      "Epoch 20/20\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 1.1395 - accuracy: 0.5752\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 1.8606 - accuracy: 0.3609\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1.8606065511703491, 0.3608750104904175]"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The results are not better when using the pretrained model. Even though it seems that the training accuracy is better due to having an increase in convergence speed due to the pretrained weights, the features learnt in the mnist dataset are not relevant to the STL_10 dataset. \n",
    "\n",
    "Mnist is smaller, (28, 28, 1), while stl 10 is (96, 96, 3). \n",
    "Mnist has images from the same class, numbers, while the classes in stl 10 are more diverse.\n",
    "\n",
    "The pretrained model clearly shows signs of overfitting. \n",
    "\n",
    "However, the transfer learning experiment was not performed well.\n",
    "In order to broadcast the stl 10 images to a shape which is also convenient for mnist, some more convolution layers were added before. \n",
    "\n",
    "maybe a better solution would have been to do upsampling for the mnist images to (96, 96, 3), because increasing the feature space might bring better results than 'forcefully' decreasing the feature space to make the 2 shapes compatible and trainable with the same model. \n"
   ],
   "metadata": {
    "id": "-xVTWAsgudco"
   }
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "_q50kLN6vlLg"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "Lab12.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}